{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in Cantones.db: [('hogar_cant',), ('poblacion_cant',), ('vivienda_cant',), ('df_renamed_table',), ('df_renamed_hogar',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Cantones.db')\n",
    "cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in Cantones.db:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/l6t22czn1d11mnzrp1kdh_340000gn/T/ipykernel_31210/958094038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cantones.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_hogar_renamed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM df_renamed_hogar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_vivienda_renamed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM df_renamed_table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpandas_sql\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    707\u001b[0m                 \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2754\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m             frame = _wrap_result(\n\u001b[0m\u001b[1;32m   2757\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_wrap_result\u001b[0;34m(data, columns, index_col, coerce_float, parse_dates, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    202\u001b[0m ):\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m\"\"\"Wrap result set of a SQLAlchemy query in a DataFrame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_arrays_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_convert_arrays_to_dataframe\u001b[0;34m(data, columns, coerce_float, dtype_backend)\u001b[0m\n\u001b[1;32m    165\u001b[0m ) -> DataFrame:\n\u001b[1;32m    166\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object_array_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     arrays = convert_object_array(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mconvert_object_array\u001b[0;34m(content, dtype, dtype_backend, coerce_float)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"O\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             arr = lib.maybe_convert_objects(\n\u001b[0m\u001b[1;32m   1031\u001b[0m                 \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                 \u001b[0mtry_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sqlite3 as sqlite3\n",
    "\n",
    "conn = sqlite3.connect('Cantones.db')\n",
    "\n",
    "df_hogar_renamed = pd.read_sql('SELECT * FROM df_renamed_hogar', conn)\n",
    "df_vivienda_renamed = pd.read_sql('SELECT * FROM df_renamed_table', conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos ambas tablas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DF shape: (2381224, 84)\n",
      "   provincia  canton_id  nro_vivienda_x  nro_hogar_x  nro_dormitorios_x  \\\n",
      "0          5          1             1.0            1                1.0   \n",
      "1          5          1             2.0            1                4.0   \n",
      "2          5          1             3.0            1                3.0   \n",
      "3          5          1             4.0            1                3.0   \n",
      "4          5          1             5.0            1                2.0   \n",
      "\n",
      "   espacio_cocina_x  tiene_inodoro_x  tiene_ducha_x  combustible_cocina_x  \\\n",
      "0               2.0              1.0            1.0                   1.0   \n",
      "1               1.0              1.0            3.0                   1.0   \n",
      "2               1.0              1.0            1.0                   1.0   \n",
      "3               2.0              1.0            3.0                   1.0   \n",
      "4               1.0              1.0            1.0                   4.0   \n",
      "\n",
      "   fuente_agua_bebida_x  ...  emigrantes_desde_2010_y  nro_emigrantes_y  \\\n",
      "0                   1.0  ...                      2.0               NaN   \n",
      "1                   1.0  ...                      2.0               NaN   \n",
      "2                   1.0  ...                      2.0               NaN   \n",
      "3                   1.0  ...                      2.0               NaN   \n",
      "4                   1.0  ...                      2.0               NaN   \n",
      "\n",
      "   total_hombres_y  total_mujeres_y  total_personas_y  \\\n",
      "0                2                1                 3   \n",
      "1                2                3                 5   \n",
      "2                1                1                 2   \n",
      "3                1                3                 4   \n",
      "4                0                1                 1   \n",
      "\n",
      "   persona_no_mencionada_y  zona_y  canton_y  nro_dormitorios_r_y  \\\n",
      "0                        2       2       501                  1.0   \n",
      "1                        2       2       501                  4.0   \n",
      "2                        2       1       501                  3.0   \n",
      "3                        2       2       501                  3.0   \n",
      "4                        2       2       501                  2.0   \n",
      "\n",
      "   registro_imputado_y  \n",
      "0                    2  \n",
      "1                    2  \n",
      "2                    2  \n",
      "3                    2  \n",
      "4                    2  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.merge(\n",
    "    df_vivienda_renamed,\n",
    "    df_hogar_renamed,\n",
    "    on=['provincia','canton_id','id_vivienda','id_hogar'],  # adjust if needed\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"\\nMerged DF shape:\", df_merged.shape)\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values by column:\n",
      " nro_emigrantes_x       2340868\n",
      "nro_emigrantes_y       2340868\n",
      "nro_fallecidos_x       2276356\n",
      "nro_fallecidos_y       2276356\n",
      "nro_gatos_x            1758342\n",
      "                        ...   \n",
      "total_mujeres_x              0\n",
      "total_hombres_x              0\n",
      "nro_hogar_x                  0\n",
      "nro_vivienda_x               0\n",
      "registro_imputado_y          0\n",
      "Length: 84, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#We take a look at the missing values\n",
    "missing_counts = df_merged.isnull().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values by column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas actuales:\n",
      "['provincia', 'canton_id', 'nro_vivienda_x', 'nro_hogar_x', 'nro_dormitorios_x', 'espacio_cocina_x', 'tiene_inodoro_x', 'tiene_ducha_x', 'combustible_cocina_x', 'fuente_agua_bebida_x', 'separa_basura_organica_x', 'separa_basura_animales_x', 'separa_basura_reciclaje_x', 'tiene_perros_x', 'nro_perros_x', 'tiene_gatos_x', 'nro_gatos_x', 'tenencia_vivienda_x', 'tiene_telefono_fijo_x', 'tiene_celular_x', 'tiene_tv_pagada_x', 'tiene_internet_x', 'tiene_computadora_x', 'tiene_refrigeradora_x', 'tiene_lavadora_x', 'tiene_secadora_x', 'tiene_microondas_x', 'tiene_extractora_olores_x', 'tiene_auto_x', 'tiene_moto_x', 'fallecidos_ultimos_3_anios_x', 'nro_fallecidos_x', 'emigrantes_desde_2010_x', 'nro_emigrantes_x', 'total_hombres_x', 'total_mujeres_x', 'total_personas_x', 'persona_no_mencionada_x', 'zona_x', 'canton_x', 'id_vivienda', 'id_hogar', 'nro_dormitorios_r_x', 'registro_imputado_x', 'nro_vivienda_y', 'nro_hogar_y', 'nro_dormitorios_y', 'espacio_cocina_y', 'tiene_inodoro_y', 'tiene_ducha_y', 'combustible_cocina_y', 'fuente_agua_bebida_y', 'separa_basura_organica_y', 'separa_basura_animales_y', 'separa_basura_reciclaje_y', 'tiene_perros_y', 'nro_perros_y', 'tiene_gatos_y', 'nro_gatos_y', 'tenencia_vivienda_y', 'tiene_telefono_fijo_y', 'tiene_celular_y', 'tiene_tv_pagada_y', 'tiene_internet_y', 'tiene_computadora_y', 'tiene_refrigeradora_y', 'tiene_lavadora_y', 'tiene_secadora_y', 'tiene_microondas_y', 'tiene_extractora_olores_y', 'tiene_auto_y', 'tiene_moto_y', 'fallecidos_ultimos_3_anios_y', 'nro_fallecidos_y', 'emigrantes_desde_2010_y', 'nro_emigrantes_y', 'total_hombres_y', 'total_mujeres_y', 'total_personas_y', 'persona_no_mencionada_y', 'zona_y', 'canton_y', 'nro_dormitorios_r_y', 'registro_imputado_y']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas actuales:\")\n",
    "print(df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing nro_vivienda_x vs nro_vivienda_y:\n",
      " → Identical across all rows? True\n",
      "Comparing nro_hogar_x vs nro_hogar_y:\n",
      " → Identical across all rows? True\n",
      "Comparing nro_dormitorios_x vs nro_dormitorios_y:\n",
      " → Identical across all rows? False\n",
      "Comparing espacio_cocina_x vs espacio_cocina_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_inodoro_x vs tiene_inodoro_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_ducha_x vs tiene_ducha_y:\n",
      " → Identical across all rows? False\n",
      "Comparing combustible_cocina_x vs combustible_cocina_y:\n",
      " → Identical across all rows? False\n",
      "Comparing fuente_agua_bebida_x vs fuente_agua_bebida_y:\n",
      " → Identical across all rows? False\n",
      "Comparing separa_basura_organica_x vs separa_basura_organica_y:\n",
      " → Identical across all rows? False\n",
      "Comparing separa_basura_animales_x vs separa_basura_animales_y:\n",
      " → Identical across all rows? False\n",
      "Comparing separa_basura_reciclaje_x vs separa_basura_reciclaje_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_perros_x vs tiene_perros_y:\n",
      " → Identical across all rows? False\n",
      "Comparing nro_perros_x vs nro_perros_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_gatos_x vs tiene_gatos_y:\n",
      " → Identical across all rows? False\n",
      "Comparing nro_gatos_x vs nro_gatos_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tenencia_vivienda_x vs tenencia_vivienda_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_telefono_fijo_x vs tiene_telefono_fijo_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_celular_x vs tiene_celular_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_tv_pagada_x vs tiene_tv_pagada_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_internet_x vs tiene_internet_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_computadora_x vs tiene_computadora_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_refrigeradora_x vs tiene_refrigeradora_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_lavadora_x vs tiene_lavadora_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_secadora_x vs tiene_secadora_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_microondas_x vs tiene_microondas_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_extractora_olores_x vs tiene_extractora_olores_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_auto_x vs tiene_auto_y:\n",
      " → Identical across all rows? False\n",
      "Comparing tiene_moto_x vs tiene_moto_y:\n",
      " → Identical across all rows? False\n",
      "Comparing fallecidos_ultimos_3_anios_x vs fallecidos_ultimos_3_anios_y:\n",
      " → Identical across all rows? False\n",
      "Comparing nro_fallecidos_x vs nro_fallecidos_y:\n",
      " → Identical across all rows? False\n",
      "Comparing emigrantes_desde_2010_x vs emigrantes_desde_2010_y:\n",
      " → Identical across all rows? False\n",
      "Comparing nro_emigrantes_x vs nro_emigrantes_y:\n",
      " → Identical across all rows? False\n",
      "Comparing total_hombres_x vs total_hombres_y:\n",
      " → Identical across all rows? True\n",
      "Comparing total_mujeres_x vs total_mujeres_y:\n",
      " → Identical across all rows? True\n",
      "Comparing total_personas_x vs total_personas_y:\n",
      " → Identical across all rows? True\n",
      "Comparing persona_no_mencionada_x vs persona_no_mencionada_y:\n",
      " → Identical across all rows? True\n",
      "Comparing zona_x vs zona_y:\n",
      " → Identical across all rows? True\n",
      "Comparing canton_x vs canton_y:\n",
      " → Identical across all rows? True\n",
      "Comparing nro_dormitorios_r_x vs nro_dormitorios_r_y:\n",
      " → Identical across all rows? False\n",
      "Comparing registro_imputado_x vs registro_imputado_y:\n",
      " → Identical across all rows? True\n"
     ]
    }
   ],
   "source": [
    "# Identify columns ending in _x and _y\n",
    "x_cols = [col for col in df_merged.columns if col.endswith('_x')]\n",
    "y_cols = [col for col in df_merged.columns if col.endswith('_y')]\n",
    "\n",
    "# For each _x column, check if there's a corresponding _y column,\n",
    "# and compare them row by row\n",
    "for x_col in x_cols:\n",
    "    # Get the base name by removing '_x' at the end\n",
    "    base_name = x_col[:-2]  \n",
    "    y_col = base_name + '_y'\n",
    "    \n",
    "    if y_col in y_cols:\n",
    "        # Check if ALL rows are identical\n",
    "        are_identical = (df_merged[x_col] == df_merged[y_col]).all()\n",
    "        \n",
    "        # Print the result\n",
    "        print(f\"Comparing {x_col} vs {y_col}:\")\n",
    "        print(\" → Identical across all rows?\", are_identical)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fully duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for entire row duplicates\n",
    "duplicates = df_merged.duplicated()\n",
    "\n",
    "# How many duplicated rows?\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of fully duplicated rows:\", num_duplicates)\n",
    "\n",
    "# (Optional) Look at the first few duplicates\n",
    "if num_duplicates > 0:\n",
    "    print(\"Sample duplicated rows:\")\n",
    "    display(df_merged[duplicates].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSIONES: \n",
    "- Al combinar las tablas tenemos columnas con informacion repetida, esto se pudo comprobar gracias a la comparación fila por fila, además, al mezclar las columnas se agregó el sufijo x y y, esto fue removido. \n",
    "- No hay columnas completamente duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping identical _x columns:\n",
      "['provincia', 'canton_id', 'id_vivienda', 'id_hogar', 'nro_vivienda_y', 'nro_hogar_y', 'nro_dormitorios_y', 'espacio_cocina_y', 'tiene_inodoro_y', 'tiene_ducha_y', 'combustible_cocina_y', 'fuente_agua_bebida_y', 'separa_basura_organica_y', 'separa_basura_animales_y', 'separa_basura_reciclaje_y', 'tiene_perros_y', 'nro_perros_y', 'tiene_gatos_y', 'nro_gatos_y', 'tenencia_vivienda_y', 'tiene_telefono_fijo_y', 'tiene_celular_y', 'tiene_tv_pagada_y', 'tiene_internet_y', 'tiene_computadora_y', 'tiene_refrigeradora_y', 'tiene_lavadora_y', 'tiene_secadora_y', 'tiene_microondas_y', 'tiene_extractora_olores_y', 'tiene_auto_y', 'tiene_moto_y', 'fallecidos_ultimos_3_anios_y', 'nro_fallecidos_y', 'emigrantes_desde_2010_y', 'nro_emigrantes_y', 'total_hombres_y', 'total_mujeres_y', 'total_personas_y', 'persona_no_mencionada_y', 'zona_y', 'canton_y', 'nro_dormitorios_r_y', 'registro_imputado_y']\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify columns ending in _x\n",
    "x_cols = [col for col in df_merged.columns if col.endswith('_x')]\n",
    "\n",
    "# 2. Loop through each _x column, find its _y counterpart, compare, drop if identical\n",
    "for x_col in x_cols:\n",
    "    # Derive the matching _y column name\n",
    "    y_col = x_col.replace('_x', '_y')\n",
    "    \n",
    "    # Only proceed if _y column actually exists\n",
    "    if y_col in df_merged.columns:\n",
    "        # Compare the entire Series to check if they're identical\n",
    "        # .equals() returns True if every element matches, incl. NaNs\n",
    "        if df_merged[x_col].equals(df_merged[y_col]):\n",
    "            df_merged.drop(columns=[x_col], inplace=True)\n",
    "\n",
    "print(\"Columns after dropping identical _x columns:\")\n",
    "print(df_merged.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el paso anterior, sacamos las columnas idénticas para sólo mantener una. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    # Identificación redundante (si ya tienes id_vivienda / id_hogar o sufijo _x)\n",
    "    'nro_vivienda_y', \n",
    "    'nro_hogar_y',\n",
    "    \n",
    "    # Manejo de residuos\n",
    "    'separa_basura_organica_y', 'separa_basura_animales_y', 'separa_basura_reciclaje_y',\n",
    "    \n",
    "    # Equipamiento muy específico (poco aporte socioeconómico comparado con otras col.)\n",
    "    'tiene_tv_pagada_y', 'tiene_microondas_y', 'tiene_extractora_olores_y', 'tiene_secadora_y',\n",
    "    \n",
    "    # Otras columnas con menor impacto en predicción\n",
    "    'persona_no_mencionada_y',\n",
    "    'registro_imputado_y'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos vulnerabilidad estructural combinando indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas actuales:\n",
      "['provincia', 'canton_id', 'id_vivienda', 'id_hogar', 'nro_dormitorios_y', 'espacio_cocina_y', 'tiene_inodoro_y', 'tiene_ducha_y', 'combustible_cocina_y', 'fuente_agua_bebida_y', 'tiene_perros_y', 'nro_perros_y', 'tiene_gatos_y', 'nro_gatos_y', 'tenencia_vivienda_y', 'tiene_telefono_fijo_y', 'tiene_celular_y', 'tiene_internet_y', 'tiene_computadora_y', 'tiene_refrigeradora_y', 'tiene_lavadora_y', 'tiene_auto_y', 'tiene_moto_y', 'fallecidos_ultimos_3_anios_y', 'nro_fallecidos_y', 'emigrantes_desde_2010_y', 'nro_emigrantes_y', 'total_hombres_y', 'total_mujeres_y', 'total_personas_y', 'zona_y', 'canton_y', 'nro_dormitorios_r_y']\n",
      "provincia                         int64\n",
      "canton_id                         int64\n",
      "id_vivienda                     float64\n",
      "id_hogar                         object\n",
      "nro_dormitorios_y               float64\n",
      "espacio_cocina_y                float64\n",
      "tiene_inodoro_y                 float64\n",
      "tiene_ducha_y                   float64\n",
      "combustible_cocina_y            float64\n",
      "fuente_agua_bebida_y            float64\n",
      "tiene_perros_y                  float64\n",
      "nro_perros_y                    float64\n",
      "tiene_gatos_y                   float64\n",
      "nro_gatos_y                     float64\n",
      "tenencia_vivienda_y             float64\n",
      "tiene_telefono_fijo_y           float64\n",
      "tiene_celular_y                 float64\n",
      "tiene_internet_y                float64\n",
      "tiene_computadora_y             float64\n",
      "tiene_refrigeradora_y           float64\n",
      "tiene_lavadora_y                float64\n",
      "tiene_auto_y                    float64\n",
      "tiene_moto_y                    float64\n",
      "fallecidos_ultimos_3_anios_y    float64\n",
      "nro_fallecidos_y                float64\n",
      "emigrantes_desde_2010_y         float64\n",
      "nro_emigrantes_y                float64\n",
      "total_hombres_y                   int64\n",
      "total_mujeres_y                   int64\n",
      "total_personas_y                  int64\n",
      "zona_y                            int64\n",
      "canton_y                          int64\n",
      "nro_dormitorios_r_y             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas actuales:\")\n",
    "print(df_merged.columns.tolist())\n",
    "print(df_merged.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'espacio_cocina_y',\n",
    "    'tiene_inodoro_y',\n",
    "    'tiene_ducha_y',\n",
    "    'combustible_cocina_y',\n",
    "    'fuente_agua_bebida_y',\n",
    "    'tiene_perros_y',\n",
    "    'tiene_gatos_y',\n",
    "    'tenencia_vivienda_y',\n",
    "    'tiene_telefono_fijo_y',\n",
    "    'tiene_celular_y',\n",
    "    'tiene_internet_y',\n",
    "    'tiene_computadora_y',\n",
    "    'tiene_refrigeradora_y',\n",
    "    'tiene_lavadora_y',\n",
    "    'tiene_auto_y',\n",
    "    'tiene_moto_y'\n",
    "    # ... Agrega otras que sean códigos sin orden numérico real\n",
    "]\n",
    "\n",
    "# 1. Conviértelas a string para que get_dummies las trate como categorías\n",
    "df[categorical_cols] = df[categorical_cols].astype('Int64').astype(str)\n",
    "\n",
    "# 2. Aplica One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "# Observa el resultado\n",
    "print(df.head())\n",
    "\n",
    "# Revisa las nuevas columnas creadas\n",
    "new_cols = [col for col in df.columns if any(prefix in col for prefix in categorical_cols)]\n",
    "print(\"\\nNuevas columnas dummies creadas:\")\n",
    "print(new_cols)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
